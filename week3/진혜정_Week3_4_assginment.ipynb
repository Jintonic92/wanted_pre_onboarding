{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week3_4 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 텐서의 크기(shape)를 계산할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n",
        "\n",
        "### Informs\n",
        "이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n",
        "\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n",
        "코드 필사를 통해 다음을 배울 수 있다.    \n",
        "- Encoder, Decoder 구조\n",
        "- Attention Mechanism\n",
        "- \"residual connection\", \"layer normalization\" 등의 구조 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoebvnNZ99r-"
      },
      "source": [
        "코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n",
        "\n",
        "최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n",
        "앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB6cNaXP99sB"
      },
      "source": [
        "Transformer 모델은 크게 4가지 클래스로 구현된다.    \n",
        "- Frame\n",
        "    - frame 역할을 하는 `EncoderDecoder` 클래스\n",
        "- Input Embedding & Encoding\n",
        "    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n",
        "- Encoder & Decoder\n",
        "    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n",
        "    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n",
        "- Sublayer\n",
        "    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n",
        "    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n",
        "    \n",
        "아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n",
        "각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaadVYo799sE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math, copy, time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEO0al299sJ"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKKyKfqB99sL"
      },
      "source": [
        "### Frame\n",
        "- `EncoderDecoder`\n",
        "\n",
        "아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n",
        " \n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n",
        "\n",
        "\n",
        "- `Generator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MECCTGpt99sP"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  ''' \n",
        "  Transformer는 Encoder-Decoder 모델을 따르며, 주요 architecture로는 stacked \n",
        "  self-sttention, point-wise fully connected layer를 사용\n",
        "  '''\n",
        "    \n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()      \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "    \n",
        "    \n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    '''masked src & target sequence를 받는다'''\n",
        "    return self.decode(self.encoder(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "    \n",
        "    \n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    \n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py2wcYPX99sT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  ''' Standard Linear 와 softmax generation step 정의'''\n",
        "  def __init__(self, d_model, vocab):\n",
        "      super(Generator, self).__init__()\n",
        "      self.proj = nn.Linear(d_model, vocab)\n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-5SRHD99sX"
      },
      "source": [
        "### Encoder\n",
        "- `Encoder`\n",
        "- `EncoderLayer`\n",
        "- `SublayerConnection`\n",
        "- Reference\n",
        "    - Layer Normalization\n",
        "        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n",
        "        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
        "    - Residual Connection\n",
        "        - [한국어 설명](https://itrepo.tistory.com/36)\n",
        "    - pytorch ModuleList\n",
        "        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjIjUBjN99sc"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "  '''N개의 동일한 layer를 만든다.'''\n",
        "  # nn.Module을 리스트로 정리하는 방법 :\n",
        "  # 각 레이어를 리스트에 전달하고 레이어 iterator 를 만드는것 \n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgglBAyM99se"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''Core Encoder는 6개 layer 층이 쌓여있다'''\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N) # nn.ModuleList([copy.deepcopy(layer) for _ in range(6)])\n",
        "    self.norm = LayerNorm(layer.size) # layer normalization layer를 추가하여 성능을 높인다\n",
        "    \n",
        "  def forward(self, x, mask):\n",
        "    ''' 각 레이어 순서대로 입력 값을 넘기며 mask를 씌운다'''\n",
        "    for layer in self.layers: # layers 들 중 each layer \n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvGP8Gsd99sg"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  \n",
        "  '''\n",
        "  2개의 sub-layer에 layer norm 적용\n",
        "  #layernorm : 각 input의 feature들에 대한 평균과 분산을 구해 batch에 있는 각 input을 정규화\n",
        "  '''\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__() \n",
        "    self.a_2 = nn.Parameter(torch.ones(features)) #features 수 만큼 '1'로 채움\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features)) #features 수 만큼 '0'로 채움\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "525_O3YE99si"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  \n",
        "  '''\n",
        "  layernorm 적용 이후 residual connection 적용\n",
        "  residual connection : 기존에 학습한 정보를 보존하고, 거기에 추가적으로 학습하는 정보 의미 \n",
        "  eg. 오픈북이 가능한 시험 \n",
        "  '''\n",
        "  def __init__(self, size, dropout):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    '''동일 사이즈의 sublayer에 residual connection 적용'''\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlGCPEVp99sk"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  '''\n",
        "  Encoder는 self attention과 feed forward로 구성 \n",
        "  '''\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    ''' Transformer Architecture의 왼쪽 부분'''\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) #multi-head attention + add & norm\n",
        "    return self.sublayer[1](x, self.feed_forward) #feed_forward + add & norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOiYmYWc99sm"
      },
      "source": [
        "### Decoder\n",
        "- `Decoder`\n",
        "- `DecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik47frFO99so"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    '''\n",
        "    Decoder 또한 6개의 layer\n",
        "    '''\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        ''' 각 레이어 순서대로 입력 값을 넘기며 mask를 씌움'''\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask) #memory key, value= encoder의 output\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElsG9P7M99sq"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    '''\n",
        "    encoder와 달리 decoder는 3번째 sublayer가 있음 \n",
        "    => encoder stack의 output에 multi-head attention을 수행\n",
        "    '''\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3) #3개의 sublayer\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        '''transformer architecture 오른쪽 부분'''\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) #masked_multi_head_attention + add & norm\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) #multi-head attention + add & norm\n",
        "        return self.sublayer[2](x, self.feed_forward) #feed_forward + add & norm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhPP8LVw99sr"
      },
      "source": [
        "### Sublayer\n",
        "- `attention` 함수\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `MultiHeadedAttention`\n",
        "- `PositionwiseFeedForward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1-iOBu99ss"
      },
      "source": [
        "### Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ochH0n99st"
      },
      "source": [
        "### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   # query 크기: (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "   # key 크기: (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "   # value 크기: (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "   # padding_mask: (batch_size, 1, 1, key의 문장 길이)"
      ],
      "metadata": {
        "id": "VapWLrBqRtzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    '''Scaled Dot Product Attention연산'''\n",
        "    # W-q, W_k, W_v 세개의 shape은 전부 (d_model, d_k)\n",
        "    # attention 함수는 주어진 Q에 대해서 모든 K와의 유사도를 각각 구한다.\n",
        "    # 이 유사도를 가중치로 하여 K와 맵핑되어 있는 각각의 V에 반영 \n",
        "    # 그리고 유사도가 반영된 V을 모두 가중합하여 return \n",
        "    d_k = query.size(-1) # d_model/num_heads\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "             #Q*K = 어텐션스코어행렬\n",
        "             #루트값으로 나눠준다\n",
        "    if mask is not None: \n",
        "    # mask : 어텐션 스코어 행렬의 마스킹할 위치에 매우 작은 음수값을 넣어준다\n",
        "    # 매우 작은 값이므로 소프트 맥스 함수를 지나면 행렬의 해당 위치 값은 0이 된다.\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    # p_attn (attention_weight) : (batch_size, num_heads, query의 문장길이, d_model/num_heads)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "        # return[0] = (batch_size, num_heads, query문장길이, d_model/num_heads)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "3dfTQVv1bOsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMYcy8h499sv"
      },
      "outputs": [],
      "source": [
        "#연습용 \n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return scores.size(), p_attn.size(), torch.matmul(p_attn, value).size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#연습용\n",
        "Q = torch.rand(12, 512)\n",
        "K = Q\n",
        "V = Q\n",
        "\n",
        "attention(Q, K, V)\n"
      ],
      "metadata": {
        "id": "5BvrCPjaZM6-",
        "outputId": "812146ab-90a1-4950-8bfc-b2e3e96c94d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([12, 12]), torch.Size([12, 12]), torch.Size([12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 연습용\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = torch.tensor([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype = torch.float32)  # (4, 3)\n",
        "\n",
        "temp_v = torch.tensor([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype = torch.float32)  # (4, 3)\n",
        "\n",
        "temp_q = torch.tensor([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype = torch.float32)  # (4, 3)\n",
        "\n",
        "attention(temp_q, temp_k, temp_v)\n"
      ],
      "metadata": {
        "id": "tr001kcrbgtf",
        "outputId": "19048582-c540-4637-cd64-e624e83d81c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 4]), torch.Size([4, 4]), torch.Size([4, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25aeigL99sw"
      },
      "source": [
        "###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "- score : (m, m)\n",
        "- p_attn : (m, m)\n",
        "- attention : (m, d_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfxLJKz99sx"
      },
      "source": [
        "### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "\n",
        "- score : torch.Size([12, 8, 1, 1])\n",
        "- p_attn : torch.Size([12, 8, 1, 1])\n",
        "- attention : torch.Size([12, 8, 1, 64])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 연습용\n",
        "\n",
        "Q = torch.rand(12, 8, 1, 64)\n",
        "K = Q\n",
        "V = Q\n",
        "nbatches = Q.size(0)\n",
        "\n",
        "attention(Q, K, V)\n"
      ],
      "metadata": {
        "id": "wufz2nRHx85P",
        "outputId": "739832b5-a275-4b3b-c77e-09ad79af0c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([12, 8, 1, 1]),\n",
              " torch.Size([12, 8, 1, 1]),\n",
              " torch.Size([12, 8, 1, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnffQE799sy"
      },
      "source": [
        "- `MultiHeadedAttention`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhFKlJ2b99sz"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        '''모델 크기 d_model과 head의 개수를 input으로 받는다'''\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # d_v = d_k 라고 항상 가정 \n",
        "        self.d_k = d_model // h #depth of model \n",
        "        self.h = h # num_heads #논문에서는 8로 지정 = 8개의 병렬 어텐션이 이루어지게 됨\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Q, K, V를 여러개의 head로 (multi head로)나눈다. \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) multi head의 attention을 구하기 위해 scaled dot production을 사용한다. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) Concat x.shape(bs, n_seq, n_head * d_head)\n",
        "        # 4) linear 과정\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 연습용\n",
        "d_model = 512\n",
        "h = 8\n",
        "d_k = d_model//h\n",
        "d_v = d_model//h\n",
        "\n",
        "Q = torch.rand(12, 512)\n",
        "K = Q\n",
        "V = Q\n",
        "nbatches = Q.size(0)\n",
        "\n",
        "hi = nn.Linear(d_model, d_model)(Q).size()\n",
        "print(hi)\n",
        "hi = nn.Linear(d_model, d_model)(Q).view(nbatches, -1, h, d_k).size()\n",
        "print(hi)\n",
        "hi = nn.Linear(d_model, d_model)(Q).view(nbatches, -1, h, d_k).transpose(1,2).size()\n",
        "print(hi)\n"
      ],
      "metadata": {
        "id": "xxDojWb7s5CJ",
        "outputId": "cfd3b4be-ef0a-4a91-f101-65af0c74e71b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 512])\n",
            "torch.Size([12, 1, 8, 64])\n",
            "torch.Size([12, 8, 1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46Ensa499s0"
      },
      "source": [
        "### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n",
        "\n",
        "- `d_k` (d_k = d_model // h) : ([1, 1])\n",
        "- `nn.Linear(d_model, d_model)(query)` : torch.Size([12, 512])\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : torch.Size([12, 1, 8, 64])\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : torch.Size([12, 8, 1, 64]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twZoeFr799s1"
      },
      "source": [
        "- `PositionwiseFeedForward`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZzpucvQ99s2"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    '''\n",
        "    Fully connectd Feed-Forward network 구성\n",
        "    - 2개의 linear layer\n",
        "    - ReLu Activation function\n",
        "    #FFN(x)=max(0,xW1+b1)W2+b2\n",
        "    '''\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff) #inner layer의 차원은 d_ff\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqjsUsbu99s3"
      },
      "source": [
        "### Input Embedding & Encoding\n",
        "- `Embeddings`\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBVJFurO99s3"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    '''\n",
        "    input/output을 d_model로 embedding함\n",
        "    linear transformation & softmax function 이용해서 \n",
        "    decoder의 output을 predicted next-token probabilities로 변경\n",
        "\n",
        "    *특이점: transformer model의 두개의 embedding layer와 이 softmax\n",
        "    function의 weight이 같다. = linear transformation에서 모두 같은 weight 사용\n",
        "    '''\n",
        "  \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po31qs_A99s5"
      },
      "source": [
        "- `PositionalEncoding`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `position` 변수 설명\n",
        "    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n",
        "- `div_term` 변수 설명\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "- `Embedding` + `Encoding` 도식화 \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP-_an3x99s5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    위치에 대한 정보 제공\n",
        "    positional encoding 차원 = embedding 차원\n",
        "    d_model, embedding vector와 더함으로써 위치 정보를 넣어준다 \n",
        "\n",
        "    '''\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 연습용\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "12iOy3bdyfZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#연습용\n",
        "position = torch.arange(0, 512).unsqueeze(1)\n",
        "print(position.size())\n",
        "d_model = 512\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "     -(math.log(10000.0) / d_model))\n",
        "print(div_term.size())\n",
        "third = position * div_term \n",
        "print(third.size())"
      ],
      "metadata": {
        "id": "Vp0_rI1cyuSZ",
        "outputId": "cdd77a60-4027-4464-e6b6-855552861c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 1])\n",
            "torch.Size([256])\n",
            "torch.Size([512, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNf13Gkm99s6"
      },
      "source": [
        "### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n",
        "\n",
        "- `position` : torch.Size([512, 1])\n",
        "- `div_term` : torch.Size([256])\n",
        "\n",
        "- `position * div_term` : torch.Size([512, 256])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rri-daP399s7"
      },
      "source": [
        "### Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZixTN199s8"
      },
      "source": [
        "### Finally Build Model\n",
        "- Xavier Initialization\n",
        "    - [한국어 자료](https://huangdi.tistory.com/8)\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdGsCiC99s8"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    '''\n",
        "    hyperaparmater설정하여 full model로 만들어주는 함수  \n",
        "    '''\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIDN1DSd99s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b5dd37-e64d-41ea-e9c7-25a0f002df66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "model = make_model(10,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljRK80Lo99s_"
      },
      "source": [
        "### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "  print(param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VAe2mARIBph",
        "outputId": "c350b66d-40c4-486b-c622-56739046e7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.layers.0.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.0.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.0.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.0.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.0.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.1.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.1.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.1.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.1.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.2.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.2.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.2.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.2.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.3.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.3.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.3.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.3.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.4.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.4.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.4.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.4.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.5.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "encoder.layers.5.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "encoder.layers.5.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "encoder.layers.5.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "encoder.norm.a_2\n",
            "torch.Size([512])\n",
            "encoder.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.0.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.0.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.0.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.1.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.1.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.1.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.2.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.2.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.2.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.3.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.3.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.3.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.4.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.4.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.4.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.0.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.0.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.1.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.1.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.2.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.3.weight\n",
            "torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.3.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.feed_forward.w_1.weight\n",
            "torch.Size([2048, 512])\n",
            "decoder.layers.5.feed_forward.w_1.bias\n",
            "torch.Size([2048])\n",
            "decoder.layers.5.feed_forward.w_2.weight\n",
            "torch.Size([512, 2048])\n",
            "decoder.layers.5.feed_forward.w_2.bias\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.b_2\n",
            "torch.Size([512])\n",
            "decoder.norm.a_2\n",
            "torch.Size([512])\n",
            "decoder.norm.b_2\n",
            "torch.Size([512])\n",
            "src_embed.0.lut.weight\n",
            "torch.Size([10, 512])\n",
            "tgt_embed.0.lut.weight\n",
            "torch.Size([10, 512])\n",
            "generator.proj.weight\n",
            "torch.Size([10, 512])\n",
            "generator.proj.bias\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHubCUOh99tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b968e6f9-ce0b-475b-c9a5-0b3252ab2ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------+------------+\n",
            "|                   Modules                   | Parameters |\n",
            "+---------------------------------------------+------------+\n",
            "| encoder.layers.0.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.0.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.0.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.0.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.0.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.0.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.0.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.0.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.0.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.0.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.0.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.0.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.0.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.0.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.0.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.0.sublayer.1.norm.b_2    |    512     |\n",
            "| encoder.layers.1.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.1.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.1.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.1.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.1.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.1.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.1.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.1.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.1.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.1.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.1.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.1.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.1.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.1.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.1.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.1.sublayer.1.norm.b_2    |    512     |\n",
            "| encoder.layers.2.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.2.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.2.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.2.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.2.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.2.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.2.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.2.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.2.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.2.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.2.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.2.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.2.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.2.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.2.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.2.sublayer.1.norm.b_2    |    512     |\n",
            "| encoder.layers.3.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.3.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.3.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.3.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.3.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.3.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.3.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.3.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.3.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.3.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.3.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.3.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.3.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.3.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.3.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.3.sublayer.1.norm.b_2    |    512     |\n",
            "| encoder.layers.4.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.4.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.4.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.4.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.4.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.4.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.4.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.4.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.4.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.4.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.4.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.4.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.4.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.4.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.4.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.4.sublayer.1.norm.b_2    |    512     |\n",
            "| encoder.layers.5.self_attn.linears.0.weight |   262144   |\n",
            "|  encoder.layers.5.self_attn.linears.0.bias  |    512     |\n",
            "| encoder.layers.5.self_attn.linears.1.weight |   262144   |\n",
            "|  encoder.layers.5.self_attn.linears.1.bias  |    512     |\n",
            "| encoder.layers.5.self_attn.linears.2.weight |   262144   |\n",
            "|  encoder.layers.5.self_attn.linears.2.bias  |    512     |\n",
            "| encoder.layers.5.self_attn.linears.3.weight |   262144   |\n",
            "|  encoder.layers.5.self_attn.linears.3.bias  |    512     |\n",
            "|   encoder.layers.5.feed_forward.w_1.weight  |  1048576   |\n",
            "|    encoder.layers.5.feed_forward.w_1.bias   |    2048    |\n",
            "|   encoder.layers.5.feed_forward.w_2.weight  |  1048576   |\n",
            "|    encoder.layers.5.feed_forward.w_2.bias   |    512     |\n",
            "|     encoder.layers.5.sublayer.0.norm.a_2    |    512     |\n",
            "|     encoder.layers.5.sublayer.0.norm.b_2    |    512     |\n",
            "|     encoder.layers.5.sublayer.1.norm.a_2    |    512     |\n",
            "|     encoder.layers.5.sublayer.1.norm.b_2    |    512     |\n",
            "|               encoder.norm.a_2              |    512     |\n",
            "|               encoder.norm.b_2              |    512     |\n",
            "| decoder.layers.0.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.0.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.0.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.0.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.0.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.0.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.0.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.0.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.0.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.0.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.0.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.0.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.0.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.0.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.0.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.0.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.0.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.0.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.0.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.0.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.0.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.0.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.0.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.0.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.0.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.0.sublayer.2.norm.b_2    |    512     |\n",
            "| decoder.layers.1.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.1.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.1.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.1.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.1.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.1.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.1.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.1.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.1.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.1.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.1.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.1.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.1.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.1.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.1.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.1.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.1.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.1.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.1.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.1.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.1.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.1.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.1.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.1.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.1.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.1.sublayer.2.norm.b_2    |    512     |\n",
            "| decoder.layers.2.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.2.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.2.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.2.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.2.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.2.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.2.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.2.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.2.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.2.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.2.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.2.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.2.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.2.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.2.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.2.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.2.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.2.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.2.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.2.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.2.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.2.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.2.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.2.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.2.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.2.sublayer.2.norm.b_2    |    512     |\n",
            "| decoder.layers.3.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.3.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.3.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.3.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.3.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.3.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.3.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.3.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.3.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.3.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.3.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.3.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.3.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.3.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.3.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.3.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.3.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.3.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.3.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.3.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.3.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.3.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.3.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.3.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.3.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.3.sublayer.2.norm.b_2    |    512     |\n",
            "| decoder.layers.4.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.4.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.4.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.4.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.4.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.4.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.4.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.4.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.4.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.4.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.4.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.4.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.4.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.4.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.4.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.4.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.4.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.4.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.4.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.4.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.4.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.4.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.4.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.4.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.4.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.4.sublayer.2.norm.b_2    |    512     |\n",
            "| decoder.layers.5.self_attn.linears.0.weight |   262144   |\n",
            "|  decoder.layers.5.self_attn.linears.0.bias  |    512     |\n",
            "| decoder.layers.5.self_attn.linears.1.weight |   262144   |\n",
            "|  decoder.layers.5.self_attn.linears.1.bias  |    512     |\n",
            "| decoder.layers.5.self_attn.linears.2.weight |   262144   |\n",
            "|  decoder.layers.5.self_attn.linears.2.bias  |    512     |\n",
            "| decoder.layers.5.self_attn.linears.3.weight |   262144   |\n",
            "|  decoder.layers.5.self_attn.linears.3.bias  |    512     |\n",
            "|  decoder.layers.5.src_attn.linears.0.weight |   262144   |\n",
            "|   decoder.layers.5.src_attn.linears.0.bias  |    512     |\n",
            "|  decoder.layers.5.src_attn.linears.1.weight |   262144   |\n",
            "|   decoder.layers.5.src_attn.linears.1.bias  |    512     |\n",
            "|  decoder.layers.5.src_attn.linears.2.weight |   262144   |\n",
            "|   decoder.layers.5.src_attn.linears.2.bias  |    512     |\n",
            "|  decoder.layers.5.src_attn.linears.3.weight |   262144   |\n",
            "|   decoder.layers.5.src_attn.linears.3.bias  |    512     |\n",
            "|   decoder.layers.5.feed_forward.w_1.weight  |  1048576   |\n",
            "|    decoder.layers.5.feed_forward.w_1.bias   |    2048    |\n",
            "|   decoder.layers.5.feed_forward.w_2.weight  |  1048576   |\n",
            "|    decoder.layers.5.feed_forward.w_2.bias   |    512     |\n",
            "|     decoder.layers.5.sublayer.0.norm.a_2    |    512     |\n",
            "|     decoder.layers.5.sublayer.0.norm.b_2    |    512     |\n",
            "|     decoder.layers.5.sublayer.1.norm.a_2    |    512     |\n",
            "|     decoder.layers.5.sublayer.1.norm.b_2    |    512     |\n",
            "|     decoder.layers.5.sublayer.2.norm.a_2    |    512     |\n",
            "|     decoder.layers.5.sublayer.2.norm.b_2    |    512     |\n",
            "|               decoder.norm.a_2              |    512     |\n",
            "|               decoder.norm.b_2              |    512     |\n",
            "|            src_embed.0.lut.weight           |    5120    |\n",
            "|            tgt_embed.0.lut.weight           |    5120    |\n",
            "|            generator.proj.weight            |    5120    |\n",
            "|             generator.proj.bias             |     10     |\n",
            "+---------------------------------------------+------------+\n",
            "Total Trainable Params: 44155914\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44155914"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# 구현\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DXpdQ1B_HqjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "진혜정 Week3_4_assginment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}